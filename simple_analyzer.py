#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–‡ä»¶ç»“æ„åˆ†æå™¨ï¼ˆæ— éœ€matplotlibï¼‰

ç”¨äºåœ¨æ²¡æœ‰å¯è§†åŒ–ä¾èµ–çš„ç¯å¢ƒä¸­ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚
"""

import os
import json
from pathlib import Path
from typing import Dict
from collections import defaultdict, Counter


class SimpleFileStructureAnalyzer:
    """ç®€åŒ–ç‰ˆæ–‡ä»¶ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        pass
    
    def load_diagnostic_report(self, report_file: str) -> Dict:
        """åŠ è½½è¯Šæ–­æŠ¥å‘Š"""
        with open(report_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def generate_comprehensive_report(self, diagnostic_report_file: str, 
                                    processing_results_file: str = None,
                                    output_dir: str = "analysis_output"):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½è¯Šæ–­æŠ¥å‘Š
        diagnostic_report = self.load_diagnostic_report(diagnostic_report_file)
        
        # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
        self.generate_text_report(diagnostic_report, processing_results_file, output_dir)
        
        print(f"\nç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆåˆ°ç›®å½•: {output_dir}")
        print(f"åŒ…å«å†…å®¹:")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {output_dir}/comprehensive_report.md")
    
    def generate_text_report(self, diagnostic_report: Dict, 
                           processing_results_file: str = None,
                           output_dir: str = "analysis_output"):
        """ç”Ÿæˆæ–‡å­—æŠ¥å‘Š"""
        report_content = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_content.append("# Starshipæ•°æ®é›†æ–‡ä»¶ç»“æ„åˆ†æç»¼åˆæŠ¥å‘Š\n")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {diagnostic_report.get('scan_summary', {}).get('scan_time', 'Unknown')}\n")
        
        # æ‰§è¡Œæ‘˜è¦
        report_content.append("## æ‰§è¡Œæ‘˜è¦\n")
        scan_summary = diagnostic_report.get('scan_summary', {})
        matching_analysis = diagnostic_report.get('matching_analysis', {})
        
        report_content.append(f"- **å‘ç°çš„starship_dataç›®å½•æ•°**: {scan_summary.get('total_starship_directories', 0)}")
        report_content.append(f"- **æ€»æ–‡ä»¶æ•°**: {scan_summary.get('total_files_scanned', 0):,}")
        report_content.append(f"- **æ•°æ®åŒ¹é…æˆåŠŸç‡**: {matching_analysis.get('success_rate', 0):.2%}")
        report_content.append(f"- **å®Œæ•´æ ·æœ¬æ•°**: {matching_analysis.get('complete_samples', 0):,}")
        report_content.append(f"- **é—®é¢˜ä¸¥é‡ç¨‹åº¦**: {'ä¸¥é‡' if matching_analysis.get('success_rate', 0) < 0.1 else 'ä¸­ç­‰' if matching_analysis.get('success_rate', 0) < 0.5 else 'è½»å¾®'}\n")
        
        # ç›®å½•ç»“æ„åˆ†æ
        report_content.append("## ç›®å½•ç»“æ„åˆ†æ\n")
        directory_structure = diagnostic_report.get('directory_structure', {})
        
        for starship_path, structure in directory_structure.items():
            report_content.append(f"### {starship_path}\n")
            
            subdirs = structure.get('subdirectories', [])
            report_content.append(f"**å‘ç°çš„å­ç›®å½•**: {', '.join(subdirs) if subdirs else 'æ— '}")
            
            file_counts = structure.get('file_counts', {})
            if file_counts:
                report_content.append("**æ–‡ä»¶æ•°é‡ç»Ÿè®¡**:")
                for subdir, count in file_counts.items():
                    report_content.append(f"  - {subdir}: {count:,}")
            
            file_extensions = structure.get('file_extensions', {})
            if file_extensions:
                report_content.append("**æ–‡ä»¶æ‰©å±•åç»Ÿè®¡**:")
                for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True)[:5]:
                    report_content.append(f"  - {ext}: {count:,}")
            
            report_content.append("")  # ç©ºè¡Œ
        
        # æ–‡ä»¶æ‰©å±•ååˆ†æ
        report_content.append("## æ–‡ä»¶ç±»å‹åˆ†æ\n")
        all_extensions = Counter()
        
        for starship_path, structure in directory_structure.items():
            file_exts = structure.get('file_extensions', {})
            for ext, count in file_exts.items():
                all_extensions[ext] += count
        
        if all_extensions:
            report_content.append("**æ€»ä½“æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ**:")
            for ext, count in all_extensions.most_common(10):
                percentage = count / sum(all_extensions.values()) * 100
                report_content.append(f"  - {ext}: {count:,} ({percentage:.1f}%)")
            
            # æŒ‰ç±»åˆ«åˆ†ç»„
            extension_categories = {
                'ç‚¹äº‘æ–‡ä»¶': ['.ply', '.pcd', '.xyz', '.pts'],
                'å›¾åƒæ–‡ä»¶': ['.png', '.jpg', '.jpeg', '.svg', '.pdf'],
                'æ•°æ®æ–‡ä»¶': ['.json', '.csv', '.txt', '.dat'],
            }
            
            category_counts = defaultdict(int)
            for ext, count in all_extensions.items():
                categorized = False
                for category, exts in extension_categories.items():
                    if ext.lower() in exts:
                        category_counts[category] += count
                        categorized = True
                        break
                if not categorized:
                    category_counts['å…¶ä»–'] += count
            
            report_content.append("\n**æŒ‰æ–‡ä»¶ç±»åˆ«åˆ†å¸ƒ**:")
            for category, count in category_counts.items():
                percentage = count / sum(all_extensions.values()) * 100
                report_content.append(f"  - {category}: {count:,} ({percentage:.1f}%)")
        
        # é—®é¢˜å’Œå»ºè®®
        report_content.append("\n## å‘ç°çš„é—®é¢˜å’Œå»ºè®®\n")
        recommendations = diagnostic_report.get('recommendations', [])
        
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                priority_emoji = "ğŸ”´" if rec['priority'] == 'HIGH' else "ğŸŸ¡" if rec['priority'] == 'MEDIUM' else "ğŸŸ¢"
                report_content.append(f"### é—®é¢˜ {i} {priority_emoji} [{rec['priority']}]")
                report_content.append(f"**ç±»å‹**: {rec['type']}")
                report_content.append(f"**æè¿°**: {rec['description']}")
                report_content.append(f"**å»ºè®®è¡ŒåŠ¨**: {rec['action']}")
                report_content.append("")
        else:
            report_content.append("æœªå‘ç°ç‰¹å®šé—®é¢˜ã€‚")
        
        # æŠ€æœ¯ç»†èŠ‚
        report_content.append("## æŠ€æœ¯ç»†èŠ‚\n")
        report_content.append("### æ–‡ä»¶åŒ¹é…é€»è¾‘åˆ†æ")
        
        if matching_analysis:
            report_content.append(f"- å‘ç°çš„å”¯ä¸€IDæ•°é‡: {matching_analysis.get('total_unique_ids', 0):,}")
            report_content.append(f"- å®Œæ•´åŒ¹é…æ ·æœ¬: {matching_analysis.get('complete_samples', 0):,}")
            report_content.append(f"- éƒ¨åˆ†åŒ¹é…æ ·æœ¬: {matching_analysis.get('partial_samples', 0):,}")
            report_content.append(f"- ä»…ç‚¹äº‘æ–‡ä»¶æ ·æœ¬: {matching_analysis.get('pointcloud_only', 0):,}")
        
        # åŠ è½½å¤„ç†ç»“æœ
        if processing_results_file and os.path.exists(processing_results_file):
            report_content.append("\n### æ•°æ®å¤„ç†ç»“æœåˆ†æ")
            with open(processing_results_file, 'r', encoding='utf-8') as f:
                processing_results = json.load(f)
            
            stats = processing_results.get('statistics', {})
            report_content.append(f"- ç‚¹äº‘æ–‡ä»¶æ•°: {stats.get('total_pointclouds_found', 0):,}")
            report_content.append(f"- å®Œæ•´æ ·æœ¬æ•°: {stats.get('complete_samples', 0):,}")
            report_content.append(f"- ä¸å®Œæ•´æ ·æœ¬æ•°: {stats.get('incomplete_samples', 0):,}")
            report_content.append(f"- å¤„ç†æˆåŠŸç‡: {stats.get('success_rate', 0):.2%}")
            report_content.append(f"- å¤„ç†æ—¶é—´: {stats.get('processing_time', 0):.2f}ç§’")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
        report_content.append("\n## ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’\n")
        success_rate = matching_analysis.get('success_rate', 0)
        
        if success_rate < 0.1:
            report_content.append("### ç´§æ€¥è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡æä½ï¼‰")
            report_content.append("1. **ç«‹å³æ£€æŸ¥æ–‡ä»¶å‘½åè§„åˆ™** - éªŒè¯å®é™…æ–‡ä»¶å‘½åæ¨¡å¼")
            report_content.append("2. **æ›´æ–°è·¯å¾„åŒ¹é…é€»è¾‘** - ä¿®æ­£æ–‡ä»¶æŸ¥æ‰¾ç®—æ³•")
            report_content.append("3. **éªŒè¯ç›®å½•ç»“æ„** - ç¡®è®¤æ‰€æœ‰å¿…è¦çš„å­ç›®å½•å­˜åœ¨")
            report_content.append("4. **æ•°æ®å®Œæ•´æ€§æ£€æŸ¥** - éªŒè¯æ•°æ®å¤åˆ¶æ˜¯å¦å®Œæ•´")
        elif success_rate < 0.5:
            report_content.append("### ä¼˜åŒ–è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡ä¸­ç­‰ï¼‰")
            report_content.append("1. **ç»†åŒ–åŒ¹é…è§„åˆ™** - æ”¹è¿›æ–‡ä»¶IDæå–é€»è¾‘")
            report_content.append("2. **å¤„ç†è¾¹ç¼˜æƒ…å†µ** - å¤„ç†ç‰¹æ®Šå‘½åæ ¼å¼")
            report_content.append("3. **éªŒè¯æ•°æ®è´¨é‡** - æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§")
        else:
            report_content.append("### ç»´æŠ¤è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡è‰¯å¥½ï¼‰")
            report_content.append("1. **ç›‘æ§æ•°æ®è´¨é‡** - å®šæœŸæ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
            report_content.append("2. **ä¼˜åŒ–å¤„ç†æ€§èƒ½** - æå‡å¤„ç†é€Ÿåº¦")
        
        # æ–‡ä»¶å‘½åæ¨¡å¼åˆ†æ
        report_content.append("\n## æ–‡ä»¶å‘½åæ¨¡å¼åˆ†æ\n")
        for starship_path, structure in directory_structure.items():
            naming_patterns = structure.get('naming_patterns', {})
            if naming_patterns:
                report_content.append(f"### {starship_path}")
                for subdir, patterns in naming_patterns.items():
                    if patterns:
                        report_content.append(f"**{subdir}ç›®å½•çš„å‘½åæ¨¡å¼**:")
                        for pattern in patterns[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ¨¡å¼
                            report_content.append(f"  - {pattern}")
                report_content.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"{output_dir}/comprehensive_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç®€åŒ–ç‰ˆæ–‡ä»¶ç»“æ„åˆ†æå·¥å…·')
    parser.add_argument('diagnostic_report', help='è¯Šæ–­æŠ¥å‘ŠJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--processing-results', help='æ•°æ®å¤„ç†ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='analysis_output', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    analyzer = SimpleFileStructureAnalyzer()
    analyzer.generate_comprehensive_report(
        args.diagnostic_report,
        args.processing_results,
        args.output_dir
    )


if __name__ == "__main__":
    main()