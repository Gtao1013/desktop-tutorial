#!/usr/bin/env python3
"""
æ–‡ä»¶ç»“æ„åˆ†æå’Œå¯è§†åŒ–å·¥å…·

æä¾›ç”¨äºåˆ†æStarshipæ•°æ®é›†æ–‡ä»¶ç»“æ„çš„å®ç”¨å‡½æ•°å’Œå¯è§†åŒ–åŠŸèƒ½ã€‚
"""

import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict, Counter
import numpy as np


class FileStructureAnalyzer:
    """æ–‡ä»¶ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®seabornæ ·å¼
        sns.set_style("whitegrid")
        sns.set_palette("husl")
    
    def load_diagnostic_report(self, report_file: str) -> Dict:
        """åŠ è½½è¯Šæ–­æŠ¥å‘Š"""
        with open(report_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def visualize_file_distribution(self, report: Dict, output_dir: str = "visualizations"):
        """å¯è§†åŒ–æ–‡ä»¶åˆ†å¸ƒ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        data = []
        for starship_path, structure in report['directory_structure'].items():
            file_counts = structure.get('file_counts', {})
            for subdir, count in file_counts.items():
                data.append({
                    'starship_directory': starship_path,
                    'subdirectory': subdir,
                    'file_count': count
                })
        
        if not data:
            print("æ²¡æœ‰æ–‡ä»¶åˆ†å¸ƒæ•°æ®å¯è§†åŒ–")
            return
        
        df = pd.DataFrame(data)
        
        # 1. æŒ‰å­ç›®å½•çš„æ–‡ä»¶æ•°é‡åˆ†å¸ƒ
        plt.figure(figsize=(12, 8))
        if not df.empty:
            subdir_counts = df.groupby('subdirectory')['file_count'].sum().sort_values(ascending=False)
            
            plt.subplot(2, 2, 1)
            subdir_counts.plot(kind='bar')
            plt.title('å„å­ç›®å½•æ–‡ä»¶æ•°é‡åˆ†å¸ƒ')
            plt.xlabel('å­ç›®å½•')
            plt.ylabel('æ–‡ä»¶æ•°é‡')
            plt.xticks(rotation=45)
            
            # 2. æŒ‰starshipç›®å½•çš„æ–‡ä»¶æ•°é‡åˆ†å¸ƒ
            plt.subplot(2, 2, 2)
            starship_counts = df.groupby('starship_directory')['file_count'].sum().sort_values(ascending=False)
            starship_counts.plot(kind='bar')
            plt.title('å„starshipç›®å½•æ–‡ä»¶æ•°é‡åˆ†å¸ƒ')
            plt.xlabel('starshipç›®å½•')
            plt.ylabel('æ–‡ä»¶æ•°é‡')
            plt.xticks(rotation=45)
            
            # 3. çƒ­åŠ›å›¾
            plt.subplot(2, 2, 3)
            pivot_table = df.pivot_table(values='file_count', 
                                       index='starship_directory', 
                                       columns='subdirectory', 
                                       fill_value=0)
            if not pivot_table.empty:
                sns.heatmap(pivot_table, annot=True, fmt='d', cmap='YlOrRd')
                plt.title('æ–‡ä»¶åˆ†å¸ƒçƒ­åŠ›å›¾')
                plt.xlabel('å­ç›®å½•')
                plt.ylabel('starshipç›®å½•')
            
            # 4. æ€»ä½“ç»Ÿè®¡
            plt.subplot(2, 2, 4)
            total_files = df['file_count'].sum()
            total_dirs = len(df['starship_directory'].unique())
            avg_files_per_dir = total_files / total_dirs if total_dirs > 0 else 0
            
            stats_text = f"""
æ€»æ–‡ä»¶æ•°: {total_files:,}
starshipç›®å½•æ•°: {total_dirs}
å¹³å‡æ¯ç›®å½•æ–‡ä»¶æ•°: {avg_files_per_dir:.1f}
æœ€å¤šæ–‡ä»¶çš„å­ç›®å½•: {subdir_counts.index[0] if not subdir_counts.empty else 'N/A'}
æœ€å¤šæ–‡ä»¶æ•°: {subdir_counts.iloc[0] if not subdir_counts.empty else 0}
            """
            plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')
            plt.axis('off')
            plt.title('ç»Ÿè®¡æ‘˜è¦')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/file_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def visualize_file_extensions(self, report: Dict, output_dir: str = "visualizations"):
        """å¯è§†åŒ–æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶æ‰©å±•å
        all_extensions = Counter()
        
        for starship_path, structure in report['directory_structure'].items():
            file_exts = structure.get('file_extensions', {})
            for ext, count in file_exts.items():
                all_extensions[ext] += count
        
        if not all_extensions:
            print("æ²¡æœ‰æ–‡ä»¶æ‰©å±•åæ•°æ®å¯è§†åŒ–")
            return
        
        # åˆ›å»ºå¯è§†åŒ–
        plt.figure(figsize=(15, 10))
        
        # 1. æ‰©å±•ååˆ†å¸ƒé¥¼å›¾
        plt.subplot(2, 2, 1)
        top_extensions = dict(all_extensions.most_common(10))
        plt.pie(top_extensions.values(), labels=top_extensions.keys(), autopct='%1.1f%%')
        plt.title('å‰10ä¸ªæ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ')
        
        # 2. æ‰©å±•åæ•°é‡æ¡å½¢å›¾
        plt.subplot(2, 2, 2)
        ext_df = pd.DataFrame(list(all_extensions.items()), columns=['extension', 'count'])
        ext_df = ext_df.sort_values('count', ascending=False).head(15)
        
        plt.bar(ext_df['extension'], ext_df['count'])
        plt.title('æ–‡ä»¶æ‰©å±•åæ•°é‡åˆ†å¸ƒï¼ˆå‰15ä¸ªï¼‰')
        plt.xlabel('æ–‡ä»¶æ‰©å±•å')
        plt.ylabel('æ–‡ä»¶æ•°é‡')
        plt.xticks(rotation=45)
        
        # 3. æŒ‰ç±»åˆ«åˆ†ç»„çš„æ‰©å±•å
        plt.subplot(2, 2, 3)
        extension_categories = {
            'ç‚¹äº‘æ–‡ä»¶': ['.ply', '.pcd', '.xyz', '.pts'],
            'å›¾åƒæ–‡ä»¶': ['.png', '.jpg', '.jpeg', '.svg', '.pdf'],
            'æ•°æ®æ–‡ä»¶': ['.json', '.csv', '.txt', '.dat'],
            'å…¶ä»–': []
        }
        
        category_counts = defaultdict(int)
        for ext, count in all_extensions.items():
            categorized = False
            for category, exts in extension_categories.items():
                if ext.lower() in exts:
                    category_counts[category] += count
                    categorized = True
                    break
            if not categorized:
                category_counts['å…¶ä»–'] += count
        
        plt.pie(category_counts.values(), labels=category_counts.keys(), autopct='%1.1f%%')
        plt.title('æŒ‰æ–‡ä»¶ç±»åˆ«åˆ†å¸ƒ')
        
        # 4. æ‰©å±•åç»Ÿè®¡è¡¨
        plt.subplot(2, 2, 4)
        stats_text = "æ–‡ä»¶æ‰©å±•åç»Ÿè®¡:\n\n"
        for ext, count in all_extensions.most_common(10):
            percentage = count / sum(all_extensions.values()) * 100
            stats_text += f"{ext:>8}: {count:>6,} ({percentage:>5.1f}%)\n"
        
        plt.text(0.1, 0.5, stats_text, fontsize=10, family='monospace', 
                verticalalignment='center')
        plt.axis('off')
        plt.title('æ‰©å±•åè¯¦ç»†ç»Ÿè®¡')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/file_extensions.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def visualize_matching_analysis(self, report: Dict, output_dir: str = "visualizations"):
        """å¯è§†åŒ–æ–‡ä»¶åŒ¹é…åˆ†æ"""
        os.makedirs(output_dir, exist_ok=True)
        
        matching = report.get('matching_analysis', {})
        if not matching:
            print("æ²¡æœ‰åŒ¹é…åˆ†ææ•°æ®å¯è§†åŒ–")
            return
        
        plt.figure(figsize=(12, 8))
        
        # 1. æ ·æœ¬å®Œæ•´æ€§åˆ†å¸ƒ
        plt.subplot(2, 2, 1)
        categories = ['å®Œæ•´æ ·æœ¬', 'éƒ¨åˆ†æ ·æœ¬', 'ä»…ç‚¹äº‘']
        values = [
            matching.get('complete_samples', 0),
            matching.get('partial_samples', 0),
            matching.get('pointcloud_only', 0)
        ]
        colors = ['green', 'orange', 'red']
        
        plt.pie(values, labels=categories, colors=colors, autopct='%1.1f%%')
        plt.title('æ ·æœ¬å®Œæ•´æ€§åˆ†å¸ƒ')
        
        # 2. æˆåŠŸç‡æŒ‡æ ‡
        plt.subplot(2, 2, 2)
        success_rate = matching.get('success_rate', 0)
        
        # åˆ›å»ºæˆåŠŸç‡ä»ªè¡¨ç›˜é£æ ¼å›¾
        angles = np.linspace(0, np.pi, 100)
        values_gauge = np.ones_like(angles)
        
        ax = plt.gca()
        ax.fill_between(angles, 0, values_gauge, alpha=0.3, color='lightgray')
        
        # æˆåŠŸç‡æŒ‡é’ˆ
        success_angle = success_rate * np.pi
        plt.arrow(0, 0, np.cos(success_angle), np.sin(success_angle), 
                 head_width=0.1, head_length=0.1, fc='red', ec='red')
        
        plt.xlim(-1.2, 1.2)
        plt.ylim(-0.2, 1.2)
        plt.title(f'æˆåŠŸç‡: {success_rate:.2%}')
        plt.axis('off')
        
        # 3. æ•°é‡å¯¹æ¯”
        plt.subplot(2, 2, 3)
        metrics = ['æ€»IDæ•°', 'å®Œæ•´æ ·æœ¬', 'éƒ¨åˆ†æ ·æœ¬', 'ä»…ç‚¹äº‘']
        values = [
            matching.get('total_unique_ids', 0),
            matching.get('complete_samples', 0),
            matching.get('partial_samples', 0),
            matching.get('pointcloud_only', 0)
        ]
        
        plt.bar(metrics, values, color=['blue', 'green', 'orange', 'red'])
        plt.title('æ ·æœ¬æ•°é‡å¯¹æ¯”')
        plt.ylabel('æ•°é‡')
        plt.xticks(rotation=45)
        
        # 4. é—®é¢˜ä¸¥é‡ç¨‹åº¦
        plt.subplot(2, 2, 4)
        total_samples = matching.get('total_unique_ids', 1)
        complete_rate = matching.get('complete_samples', 0) / total_samples
        partial_rate = matching.get('partial_samples', 0) / total_samples
        missing_rate = matching.get('pointcloud_only', 0) / total_samples
        
        severity_text = f"""
æ•°æ®è´¨é‡è¯„ä¼°:

æ€»æ ·æœ¬æ•°: {total_samples:,}
å®Œæ•´ç‡: {complete_rate:.2%}
éƒ¨åˆ†å®Œæ•´ç‡: {partial_rate:.2%}
ä¸¥é‡ç¼ºå¤±ç‡: {missing_rate:.2%}

è´¨é‡è¯„çº§: {'ä¼˜ç§€' if complete_rate > 0.9 else 'è‰¯å¥½' if complete_rate > 0.7 else 'ä¸€èˆ¬' if complete_rate > 0.5 else 'è¾ƒå·®' if complete_rate > 0.2 else 'ä¸¥é‡é—®é¢˜'}
        """
        
        plt.text(0.1, 0.5, severity_text, fontsize=11, verticalalignment='center')
        plt.axis('off')
        plt.title('æ•°æ®è´¨é‡è¯„ä¼°')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/matching_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_comprehensive_report(self, diagnostic_report_file: str, 
                                    processing_results_file: str = None,
                                    output_dir: str = "analysis_output"):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½è¯Šæ–­æŠ¥å‘Š
        diagnostic_report = self.load_diagnostic_report(diagnostic_report_file)
        
        # ç”Ÿæˆå¯è§†åŒ–
        viz_dir = f"{output_dir}/visualizations"
        self.visualize_file_distribution(diagnostic_report, viz_dir)
        self.visualize_file_extensions(diagnostic_report, viz_dir)
        self.visualize_matching_analysis(diagnostic_report, viz_dir)
        
        # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
        self.generate_text_report(diagnostic_report, processing_results_file, output_dir)
        
        print(f"\nç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆåˆ°ç›®å½•: {output_dir}")
        print(f"åŒ…å«å†…å®¹:")
        print(f"  - å¯è§†åŒ–å›¾è¡¨: {viz_dir}/")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {output_dir}/comprehensive_report.md")
    
    def generate_text_report(self, diagnostic_report: Dict, 
                           processing_results_file: str = None,
                           output_dir: str = "analysis_output"):
        """ç”Ÿæˆæ–‡å­—æŠ¥å‘Š"""
        report_content = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_content.append("# Starshipæ•°æ®é›†æ–‡ä»¶ç»“æ„åˆ†æç»¼åˆæŠ¥å‘Š\n")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {diagnostic_report.get('scan_summary', {}).get('scan_time', 'Unknown')}\n")
        
        # æ‰§è¡Œæ‘˜è¦
        report_content.append("## æ‰§è¡Œæ‘˜è¦\n")
        scan_summary = diagnostic_report.get('scan_summary', {})
        matching_analysis = diagnostic_report.get('matching_analysis', {})
        
        report_content.append(f"- **å‘ç°çš„starship_dataç›®å½•æ•°**: {scan_summary.get('total_starship_directories', 0)}")
        report_content.append(f"- **æ€»æ–‡ä»¶æ•°**: {scan_summary.get('total_files_scanned', 0):,}")
        report_content.append(f"- **æ•°æ®åŒ¹é…æˆåŠŸç‡**: {matching_analysis.get('success_rate', 0):.2%}")
        report_content.append(f"- **å®Œæ•´æ ·æœ¬æ•°**: {matching_analysis.get('complete_samples', 0):,}")
        report_content.append(f"- **é—®é¢˜ä¸¥é‡ç¨‹åº¦**: {'ä¸¥é‡' if matching_analysis.get('success_rate', 0) < 0.1 else 'ä¸­ç­‰' if matching_analysis.get('success_rate', 0) < 0.5 else 'è½»å¾®'}\n")
        
        # ç›®å½•ç»“æ„åˆ†æ
        report_content.append("## ç›®å½•ç»“æ„åˆ†æ\n")
        directory_structure = diagnostic_report.get('directory_structure', {})
        
        for starship_path, structure in directory_structure.items():
            report_content.append(f"### {starship_path}\n")
            
            subdirs = structure.get('subdirectories', [])
            report_content.append(f"**å‘ç°çš„å­ç›®å½•**: {', '.join(subdirs) if subdirs else 'æ— '}")
            
            file_counts = structure.get('file_counts', {})
            if file_counts:
                report_content.append("**æ–‡ä»¶æ•°é‡ç»Ÿè®¡**:")
                for subdir, count in file_counts.items():
                    report_content.append(f"  - {subdir}: {count:,}")
            
            file_extensions = structure.get('file_extensions', {})
            if file_extensions:
                report_content.append("**æ–‡ä»¶æ‰©å±•åç»Ÿè®¡**:")
                for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True)[:5]:
                    report_content.append(f"  - {ext}: {count:,}")
            
            report_content.append("")  # ç©ºè¡Œ
        
        # é—®é¢˜å’Œå»ºè®®
        report_content.append("## å‘ç°çš„é—®é¢˜å’Œå»ºè®®\n")
        recommendations = diagnostic_report.get('recommendations', [])
        
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                priority_emoji = "ğŸ”´" if rec['priority'] == 'HIGH' else "ğŸŸ¡" if rec['priority'] == 'MEDIUM' else "ğŸŸ¢"
                report_content.append(f"### é—®é¢˜ {i} {priority_emoji} [{rec['priority']}]")
                report_content.append(f"**ç±»å‹**: {rec['type']}")
                report_content.append(f"**æè¿°**: {rec['description']}")
                report_content.append(f"**å»ºè®®è¡ŒåŠ¨**: {rec['action']}")
                report_content.append("")
        else:
            report_content.append("æœªå‘ç°ç‰¹å®šé—®é¢˜ã€‚")
        
        # æŠ€æœ¯ç»†èŠ‚
        report_content.append("## æŠ€æœ¯ç»†èŠ‚\n")
        report_content.append("### æ–‡ä»¶åŒ¹é…é€»è¾‘åˆ†æ")
        
        if matching_analysis:
            report_content.append(f"- å‘ç°çš„å”¯ä¸€IDæ•°é‡: {matching_analysis.get('total_unique_ids', 0):,}")
            report_content.append(f"- å®Œæ•´åŒ¹é…æ ·æœ¬: {matching_analysis.get('complete_samples', 0):,}")
            report_content.append(f"- éƒ¨åˆ†åŒ¹é…æ ·æœ¬: {matching_analysis.get('partial_samples', 0):,}")
            report_content.append(f"- ä»…ç‚¹äº‘æ–‡ä»¶æ ·æœ¬: {matching_analysis.get('pointcloud_only', 0):,}")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
        report_content.append("\n## ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’\n")
        success_rate = matching_analysis.get('success_rate', 0)
        
        if success_rate < 0.1:
            report_content.append("### ç´§æ€¥è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡æä½ï¼‰")
            report_content.append("1. **ç«‹å³æ£€æŸ¥æ–‡ä»¶å‘½åè§„åˆ™** - éªŒè¯å®é™…æ–‡ä»¶å‘½åæ¨¡å¼")
            report_content.append("2. **æ›´æ–°è·¯å¾„åŒ¹é…é€»è¾‘** - ä¿®æ­£æ–‡ä»¶æŸ¥æ‰¾ç®—æ³•")
            report_content.append("3. **éªŒè¯ç›®å½•ç»“æ„** - ç¡®è®¤æ‰€æœ‰å¿…è¦çš„å­ç›®å½•å­˜åœ¨")
            report_content.append("4. **æ•°æ®å®Œæ•´æ€§æ£€æŸ¥** - éªŒè¯æ•°æ®å¤åˆ¶æ˜¯å¦å®Œæ•´")
        elif success_rate < 0.5:
            report_content.append("### ä¼˜åŒ–è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡ä¸­ç­‰ï¼‰")
            report_content.append("1. **ç»†åŒ–åŒ¹é…è§„åˆ™** - æ”¹è¿›æ–‡ä»¶IDæå–é€»è¾‘")
            report_content.append("2. **å¤„ç†è¾¹ç¼˜æƒ…å†µ** - å¤„ç†ç‰¹æ®Šå‘½åæ ¼å¼")
            report_content.append("3. **éªŒè¯æ•°æ®è´¨é‡** - æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§")
        else:
            report_content.append("### ç»´æŠ¤è¡ŒåŠ¨ï¼ˆæˆåŠŸç‡è‰¯å¥½ï¼‰")
            report_content.append("1. **ç›‘æ§æ•°æ®è´¨é‡** - å®šæœŸæ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
            report_content.append("2. **ä¼˜åŒ–å¤„ç†æ€§èƒ½** - æå‡å¤„ç†é€Ÿåº¦")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"{output_dir}/comprehensive_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–‡ä»¶ç»“æ„åˆ†æå’Œå¯è§†åŒ–å·¥å…·')
    parser.add_argument('diagnostic_report', help='è¯Šæ–­æŠ¥å‘ŠJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--processing-results', help='æ•°æ®å¤„ç†ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='analysis_output', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    analyzer = FileStructureAnalyzer()
    analyzer.generate_comprehensive_report(
        args.diagnostic_report,
        args.processing_results,
        args.output_dir
    )


if __name__ == "__main__":
    main()